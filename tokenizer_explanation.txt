//tokenizer explanation
A prompt is delivered to a tokenizer, which tokenizes each word, or returns a set of integers for each. This tokenization process determines the focus of the prompt, which then moves us to the mixture of experts service. The mixture of experts service is a set of various domain specific large language models. These domain specific large language models are trained on data sets specific to a certain subject. Think of this process as a tree, with the LLM that you are inputting the query to at the top, and the different specialized "expert" LLMS  below it. Different queries being outsourced to different specialized LLMs, which process these queries and generate answers. Looking at this through a coding assistance lens, I may input a query into Gemini to debug my java code. My query is then tokenized, and the mixture of experts service will determine that my query is best served by an LLM that was trained on java code, and not another LLM which was trained on python code. This LLM will then generate a response, which is what I will end up seeing on my screen.
